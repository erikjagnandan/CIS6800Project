{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "_The following code requires having uploaded the dinov2 folder from our GitHub repository to your My Drive folder on Google Drive. There are further instructions below._"
      ],
      "metadata": {
        "id": "zTAb3Pf2pPJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Run the first 2 code blocks. After the second code block terminates, it will ask you to restart the session. Restart the session and then proceed to the next code block. You do not need to re-run the first two blocks after restarting the session._"
      ],
      "metadata": {
        "id": "Du4ygr73pSNw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YejRIXLjOZwh",
        "outputId": "edf5558f-a78b-446f-a47e-2d8e54e4c09a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "n4iJILqVOmWC",
        "outputId": "74cefc0d-2b14-481d-b93d-8fd9a2e1bf8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openmim\n",
            "  Downloading openmim-0.3.9-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: Click in /usr/local/lib/python3.10/dist-packages (from openmim) (8.1.7)\n",
            "Collecting colorama (from openmim)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting model-index (from openmim)\n",
            "  Downloading model_index-0.1.11-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting opendatalab (from openmim)\n",
            "  Downloading opendatalab-0.0.10-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from openmim) (2.2.2)\n",
            "Requirement already satisfied: pip>=19.3 in /usr/local/lib/python3.10/dist-packages (from openmim) (24.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from openmim) (2.32.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from openmim) (13.9.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from openmim) (0.9.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from model-index->openmim) (6.0.2)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from model-index->openmim) (3.7)\n",
            "Collecting ordered-set (from model-index->openmim)\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting pycryptodome (from opendatalab->openmim)\n",
            "  Downloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatalab->openmim) (4.66.6)\n",
            "Collecting openxlab (from opendatalab->openmim)\n",
            "  Downloading openxlab-0.1.2-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (2024.8.30)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (2024.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim) (4.12.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->openmim) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->openmim) (1.17.0)\n",
            "Collecting filelock~=3.14.0 (from openxlab->opendatalab->openmim)\n",
            "  Downloading filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting oss2~=2.17.0 (from openxlab->opendatalab->openmim)\n",
            "  Downloading oss2-2.17.0.tar.gz (259 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging~=24.0 in /usr/local/lib/python3.10/dist-packages (from openxlab->opendatalab->openmim) (24.2)\n",
            "Collecting pytz>=2020.1 (from pandas->openmim)\n",
            "  Downloading pytz-2023.4-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting requests (from openmim)\n",
            "  Downloading requests-2.28.2-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting rich (from openmim)\n",
            "  Downloading rich-13.4.2-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting setuptools~=60.2.0 (from openxlab->opendatalab->openmim)\n",
            "  Downloading setuptools-60.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting tqdm (from opendatalab->openmim)\n",
            "  Downloading tqdm-4.65.2-py3-none-any.whl.metadata (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.21.1 (from requests->openmim)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting crcmod>=1.7 (from oss2~=2.17.0->openxlab->opendatalab->openmim)\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting aliyun-python-sdk-kms>=2.4.1 (from oss2~=2.17.0->openxlab->opendatalab->openmim)\n",
            "  Downloading aliyun_python_sdk_kms-2.16.5-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting aliyun-python-sdk-core>=2.13.12 (from oss2~=2.17.0->openxlab->opendatalab->openmim)\n",
            "  Downloading aliyun-python-sdk-core-2.16.0.tar.gz (449 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m449.6/449.6 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jmespath<1.0.0,>=0.9.3 (from aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim)\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: cryptography>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.0.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.0.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (2.22)\n",
            "Downloading openmim-0.3.9-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading model_index-0.1.11-py3-none-any.whl (34 kB)\n",
            "Downloading opendatalab-0.0.10-py3-none-any.whl (29 kB)\n",
            "Downloading openxlab-0.1.2-py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.5/311.5 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich-13.4.2-py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.4/239.4 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2023.4-py2.py3-none-any.whl (506 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.5/506.5 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.65.2-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Downloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.14.0-py3-none-any.whl (12 kB)\n",
            "Downloading setuptools-60.2.0-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.1/953.1 kB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aliyun_python_sdk_kms-2.16.5-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.5/99.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: oss2, aliyun-python-sdk-core, crcmod\n",
            "  Building wheel for oss2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for oss2: filename=oss2-2.17.0-py3-none-any.whl size=112372 sha256=8e792f25d69c9cd0c3a7d417a2ba0d162ffc531ec379c39bd9da4ceea6f7e674\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/04/7b/7e61b8157fdf211c5131375240d0d86ca82e2a88ead9672c88\n",
            "  Building wheel for aliyun-python-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for aliyun-python-sdk-core: filename=aliyun_python_sdk_core-2.16.0-py3-none-any.whl size=535316 sha256=a82d3ec0ecfc41c3adcf97a470ce46cfc320cd1d4a363a688ced4e0b363be3ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/11/5e/08e7cb4e03a3e83b4862edd12d1143c8d3936a3dd57a3ee46d\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp310-cp310-linux_x86_64.whl size=31406 sha256=3755a2a5fc9faecd5aa065d86776ab559b9d7812847fdbf943cdecd984ba5ada\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/4c/07/72215c529bd59d67e3dac29711d7aba1b692f543c808ba9e86\n",
            "Successfully built oss2 aliyun-python-sdk-core crcmod\n",
            "Installing collected packages: pytz, crcmod, urllib3, tqdm, setuptools, pycryptodome, ordered-set, jmespath, filelock, colorama, rich, requests, model-index, aliyun-python-sdk-core, aliyun-python-sdk-kms, oss2, openxlab, opendatalab, openmim\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2024.2\n",
            "    Uninstalling pytz-2024.2:\n",
            "      Successfully uninstalled pytz-2024.2\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.2.3\n",
            "    Uninstalling urllib3-2.2.3:\n",
            "      Successfully uninstalled urllib3-2.2.3\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.6\n",
            "    Uninstalling tqdm-4.66.6:\n",
            "      Successfully uninstalled tqdm-4.66.6\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.1.0\n",
            "    Uninstalling setuptools-75.1.0:\n",
            "      Successfully uninstalled setuptools-75.1.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.16.1\n",
            "    Uninstalling filelock-3.16.1:\n",
            "      Successfully uninstalled filelock-3.16.1\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.9.4\n",
            "    Uninstalling rich-13.9.4:\n",
            "      Successfully uninstalled rich-13.9.4\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.28.2 which is incompatible.\n",
            "pymc 5.18.2 requires rich>=13.7.1, but you have rich 13.4.2 which is incompatible.\n",
            "pytensor 2.26.4 requires filelock>=3.15, but you have filelock 3.14.0 which is incompatible.\n",
            "sphinx 8.1.3 requires requests>=2.30.0, but you have requests 2.28.2 which is incompatible.\n",
            "yfinance 0.2.50 requires requests>=2.31, but you have requests 2.28.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aliyun-python-sdk-core-2.16.0 aliyun-python-sdk-kms-2.16.5 colorama-0.4.6 crcmod-1.7 filelock-3.14.0 jmespath-0.10.0 model-index-0.1.11 opendatalab-0.0.10 openmim-0.3.9 openxlab-0.1.2 ordered-set-4.1.0 oss2-2.17.0 pycryptodome-3.21.0 pytz-2023.4 requests-2.28.2 rich-13.4.2 setuptools-60.2.0 tqdm-4.65.2 urllib3-1.26.20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "setuptools"
                ]
              },
              "id": "26452092e5a14594aab5282c3164d659"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -U openmim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_The next two blocks require some time to run (1-3 minutes total). Run them, and while they are running, do the following:_\n",
        "\n",
        "_Upload all images and ground truth depth maps to the /content directory. If you click on the Files tab to the left, you should be brought to the /content directory by default (you should see directories called .config and sample\\_data). For example, your file system could look like /content/1.png, /content/1.jpg, /content/2.png, etc._\n",
        "\n",
        "_This code is designed to work for one scene at a time, so only upload the data for one scene. Performance metrics for multiple scenes can be easily obtained by running this code separately for each scene, and then taking the average of the performance on each scene, weighted by the number of samples in each scene._\n",
        "\n",
        "_The outputs in this notebook were computed specifically on the scene basement\\_0001a\\_out, whereas the performance reported in the paper is averaged across all scenes, so the performance metrics here will not perfectly match those in the paper._"
      ],
      "metadata": {
        "id": "xDWhBUAUpaVi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52eDNTZNOqUH",
        "outputId": "4a9a7ef1-8688-421e-8284-02fc655ed3fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.openmmlab.com/mmcv/dist/cu121/torch2.5.0/index.html\n",
            "Collecting mmcv==1.5.0\n",
            "  Downloading mmcv-1.5.0.tar.gz (530 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m530.7/530.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting addict (from mmcv==1.5.0)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mmcv==1.5.0) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mmcv==1.5.0) (24.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from mmcv==1.5.0) (11.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from mmcv==1.5.0) (6.0.2)\n",
            "Collecting yapf (from mmcv==1.5.0)\n",
            "  Downloading yapf-0.43.0-py3-none-any.whl.metadata (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf->mmcv==1.5.0) (4.3.6)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->mmcv==1.5.0) (2.2.1)\n",
            "Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading yapf-0.43.0-py3-none-any.whl (256 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: mmcv\n",
            "  Building wheel for mmcv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mmcv: filename=mmcv-1.5.0-py2.py3-none-any.whl size=807180 sha256=3294d3cd85064a99a3197334c79b95797e77e2f71d4186ab2a7e33808f96c1a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/b4/5d/1250f6319cd64acea208a8cd5a3e600506381c05bd65343d22\n",
            "Successfully built mmcv\n",
            "Installing collected packages: addict, yapf, mmcv\n",
            "Successfully installed addict-2.4.0 mmcv-1.5.0 yapf-0.43.0\n"
          ]
        }
      ],
      "source": [
        "!mim install mmcv==1.5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "8lkavS62OsQy",
        "outputId": "a9fe63b7-70c6-4534-982c-075a242bc646"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/dinov2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "# Define the source and destination path\n",
        "source_folder_path = '/content/gdrive/My Drive/dinov2'\n",
        "destination_folder_path = '/content/dinov2'\n",
        "\n",
        "# Copy the folder\n",
        "shutil.copytree(source_folder_path, destination_folder_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Run the next 5 blocks_"
      ],
      "metadata": {
        "id": "q0SshmFAtEYz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VwZU17pOu7n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import math\n",
        "import itertools\n",
        "from functools import partial\n",
        "import mmcv\n",
        "from collections import defaultdict\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "from dinov2.eval.depth.models import build_depther"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPXPO_M_O4IX"
      },
      "outputs": [],
      "source": [
        "class CenterPadding(torch.nn.Module):\n",
        "    def __init__(self, multiple):\n",
        "        super().__init__()\n",
        "        self.multiple = multiple\n",
        "\n",
        "    def _get_pad(self, size):\n",
        "        new_size = math.ceil(size / self.multiple) * self.multiple\n",
        "        pad_size = new_size - size\n",
        "        pad_size_left = pad_size // 2\n",
        "        pad_size_right = pad_size - pad_size_left\n",
        "        return pad_size_left, pad_size_right\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def forward(self, x):\n",
        "        pads = list(itertools.chain.from_iterable(self._get_pad(m) for m in x.shape[:1:-1]))\n",
        "        output = F.pad(x, pads)\n",
        "        return output\n",
        "\n",
        "\n",
        "def create_depther(cfg, backbone_model, backbone_size, head_type):\n",
        "    train_cfg = cfg.get(\"train_cfg\")\n",
        "    test_cfg = cfg.get(\"test_cfg\")\n",
        "    depther = build_depther(cfg.model, train_cfg=train_cfg, test_cfg=test_cfg)\n",
        "\n",
        "    depther.backbone.forward = partial(\n",
        "        backbone_model.get_intermediate_layers,\n",
        "        n=cfg.model.backbone.out_indices,\n",
        "        reshape=True,\n",
        "        return_class_token=cfg.model.backbone.output_cls_token,\n",
        "        norm=cfg.model.backbone.final_norm,\n",
        "    )\n",
        "\n",
        "    if hasattr(backbone_model, \"patch_size\"):\n",
        "        depther.backbone.register_forward_pre_hook(lambda _, x: CenterPadding(backbone_model.patch_size)(x[0]))\n",
        "\n",
        "    return depther"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_vBjw7iO7yU"
      },
      "outputs": [],
      "source": [
        "def make_depth_transform() -> transforms.Compose:\n",
        "    return transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        lambda x: 255.0 * x[:3], # Discard alpha component and scale by 255\n",
        "        transforms.Normalize(\n",
        "            mean=(123.675, 116.28, 103.53),\n",
        "            std=(58.395, 57.12, 57.375),\n",
        "        ),\n",
        "    ])\n",
        "\n",
        "\n",
        "def render_depth(values, colormap_name=\"magma_r\") -> Image:\n",
        "    min_value, max_value = values.min(), values.max()\n",
        "    normalized_values = (values - min_value) / (max_value - min_value)\n",
        "\n",
        "    colormap = matplotlib.colormaps[colormap_name]\n",
        "    colors = colormap(normalized_values, bytes=True) # ((1)xhxwx4)\n",
        "    colors = colors[:, :, :3] # Discard alpha component\n",
        "    return Image.fromarray(colors)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_If you wish to change the size of the DINOv2 encoder, set BACKBONE\\_SIZE accordingly._"
      ],
      "metadata": {
        "id": "3BedAfn4tcrp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQUnzoMtO9qa",
        "outputId": "0dea05d9-a134-405e-9078-7b0854f2610a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/dinov2/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "/content/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
            "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
            "/content/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
            "  warnings.warn(\"xFormers is not available (Attention)\")\n",
            "/content/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
            "  warnings.warn(\"xFormers is not available (Block)\")\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dinov2_vits14_pretrain.pth\n",
            "100%|██████████| 84.2M/84.2M [00:01<00:00, 87.8MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DinoVisionTransformer(\n",
              "  (patch_embed): PatchEmbed(\n",
              "    (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
              "    (norm): Identity()\n",
              "  )\n",
              "  (blocks): ModuleList(\n",
              "    (0-11): 12 x NestedTensorBlock(\n",
              "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): MemEffAttention(\n",
              "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): LayerScale()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): LayerScale()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "  (head): Identity()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "BACKBONE_SIZE = \"small\" # in (\"small\", \"base\", \"large\" or \"giant\")\n",
        "\n",
        "\n",
        "backbone_archs = {\n",
        "    \"small\": \"vits14\",\n",
        "    \"base\": \"vitb14\",\n",
        "    \"large\": \"vitl14\",\n",
        "    \"giant\": \"vitg14\",\n",
        "}\n",
        "backbone_arch = backbone_archs[BACKBONE_SIZE]\n",
        "backbone_name = f\"dinov2_{backbone_arch}\"\n",
        "\n",
        "backbone_model = torch.hub.load(repo_or_dir=\"facebookresearch/dinov2\", model=backbone_name)\n",
        "backbone_model.eval()\n",
        "backbone_model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oaHsCwfPB2T",
        "outputId": "f9f85de5-1ad7-441d-d0c1-ce75775df828"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_nyu_dpt_head.pth\" to /root/.cache/torch/hub/checkpoints/dinov2_vits14_nyu_dpt_head.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load checkpoint from http path: https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_nyu_dpt_head.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 160M/160M [00:01<00:00, 119MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DepthEncoderDecoder(\n",
              "  (backbone): DinoVisionTransformer()\n",
              "  (decode_head): DPTHead(\n",
              "    align_corners=False\n",
              "    (loss_decode): ModuleList(\n",
              "      (0): SigLoss()\n",
              "      (1): GradientLoss()\n",
              "    )\n",
              "    (conv_depth): HeadDepth(\n",
              "      (head): Sequential(\n",
              "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): Interpolate()\n",
              "        (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (3): ReLU()\n",
              "        (4): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (relu): ReLU()\n",
              "    (sigmoid): Sigmoid()\n",
              "    (reassemble_blocks): ReassembleBlocks(\n",
              "      (projects): ModuleList(\n",
              "        (0): ConvModule(\n",
              "          (conv): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): ConvModule(\n",
              "          (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): ConvModule(\n",
              "          (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (3): ConvModule(\n",
              "          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (resize_layers): ModuleList(\n",
              "        (0): ConvTranspose2d(48, 48, kernel_size=(4, 4), stride=(4, 4))\n",
              "        (1): ConvTranspose2d(96, 96, kernel_size=(2, 2), stride=(2, 2))\n",
              "        (2): Identity()\n",
              "        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      )\n",
              "      (readout_projects): ModuleList(\n",
              "        (0-3): 4 x Sequential(\n",
              "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (convs): ModuleList(\n",
              "      (0): ConvModule(\n",
              "        (conv): Conv2d(48, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (1): ConvModule(\n",
              "        (conv): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (2): ConvModule(\n",
              "        (conv): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (3): ConvModule(\n",
              "        (conv): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (fusion_blocks): ModuleList(\n",
              "      (0): FeatureFusionBlock(\n",
              "        (project): ConvModule(\n",
              "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (res_conv_unit1): None\n",
              "        (res_conv_unit2): PreActResidualConvUnit(\n",
              "          (conv1): ConvModule(\n",
              "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (activate): ReLU(inplace=True)\n",
              "          )\n",
              "          (conv2): ConvModule(\n",
              "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (activate): ReLU(inplace=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-3): 3 x FeatureFusionBlock(\n",
              "        (project): ConvModule(\n",
              "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (res_conv_unit1): PreActResidualConvUnit(\n",
              "          (conv1): ConvModule(\n",
              "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (activate): ReLU(inplace=True)\n",
              "          )\n",
              "          (conv2): ConvModule(\n",
              "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (activate): ReLU(inplace=True)\n",
              "          )\n",
              "        )\n",
              "        (res_conv_unit2): PreActResidualConvUnit(\n",
              "          (conv1): ConvModule(\n",
              "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (activate): ReLU(inplace=True)\n",
              "          )\n",
              "          (conv2): ConvModule(\n",
              "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (activate): ReLU(inplace=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (project): ConvModule(\n",
              "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (activate): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import urllib\n",
        "\n",
        "import mmcv\n",
        "from mmcv.runner import load_checkpoint\n",
        "\n",
        "\n",
        "def load_config_from_url(url: str) -> str:\n",
        "    with urllib.request.urlopen(url) as f:\n",
        "        return f.read().decode()\n",
        "\n",
        "\n",
        "HEAD_DATASET = \"nyu\" # in (\"nyu\", \"kitti\")\n",
        "HEAD_TYPE = \"dpt\" # in (\"linear\", \"linear4\", \"dpt\")\n",
        "\n",
        "\n",
        "DINOV2_BASE_URL = \"https://dl.fbaipublicfiles.com/dinov2\"\n",
        "head_config_url = f\"{DINOV2_BASE_URL}/{backbone_name}/{backbone_name}_{HEAD_DATASET}_{HEAD_TYPE}_config.py\"\n",
        "head_checkpoint_url = f\"{DINOV2_BASE_URL}/{backbone_name}/{backbone_name}_{HEAD_DATASET}_{HEAD_TYPE}_head.pth\"\n",
        "\n",
        "cfg_str = load_config_from_url(head_config_url)\n",
        "cfg = mmcv.Config.fromstring(cfg_str, file_format=\".py\")\n",
        "\n",
        "model = create_depther(\n",
        "    cfg,\n",
        "    backbone_model=backbone_model,\n",
        "    backbone_size=BACKBONE_SIZE,\n",
        "    head_type=HEAD_TYPE,\n",
        ")\n",
        "\n",
        "load_checkpoint(model, head_checkpoint_url, map_location=\"cpu\")\n",
        "model.eval()\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Set num\\_images to the number of images for the scene you uploaded_"
      ],
      "metadata": {
        "id": "_7S0wxjttH4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_images = 281"
      ],
      "metadata": {
        "id": "gVEJIVTXrRPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Run this block to set up the speed tests._"
      ],
      "metadata": {
        "id": "cHB44YVYtN0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Speed Test Setup\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "%cd '/content/'\n",
        "\n",
        "spatial_offset_pixels = 150\n",
        "distance_threshold = 100\n",
        "feature_algorithm = cv2.ORB_create(nfeatures=500)\n",
        "\n",
        "image_height = 480\n",
        "image_width = 640\n",
        "original_dino_depth_map = torch.zeros((num_images, image_height, image_width))\n",
        "updated_dino_depth_map = torch.zeros((num_images, image_height, image_width))\n",
        "images = []\n",
        "transformed_images = []\n",
        "grayscale_images = []\n",
        "\n",
        "transform = make_depth_transform()\n",
        "\n",
        "for i in range(num_images):\n",
        "    images.append(cv2.imread(str(i+1) + '.jpg'))\n",
        "    transformed_images.append(transform(images[i]).unsqueeze(0).cuda())\n",
        "    grayscale_images.append(cv2.imread(str(i+1) + '.jpg', 0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlZqmQQ3V5W6",
        "outputId": "2f801b07-f7c2-4bbf-d874-b36cff5fee14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Run this block to test the speed of the Vanilla DINOv2 model._"
      ],
      "metadata": {
        "id": "g9z8W2tutXSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Speed Test - Vanilla Model\n",
        "\n",
        "start = torch.cuda.Event(enable_timing=True)\n",
        "end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "start.record()\n",
        "for i in range(num_images):\n",
        "    with torch.no_grad():\n",
        "        original_dino_depth_map[i] = model.whole_inference(transformed_images[i], img_meta=None, rescale=True).squeeze()\n",
        "\n",
        "end.record()\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "avg_milliseconds_original = start.elapsed_time(end)/num_images\n",
        "print(\"Vanilla DINOv2 Time for \" + str(num_images) + \" Forward Props = \" + str(avg_milliseconds_original) + \" milliseconds (\" + str(1000/avg_milliseconds_original) + \" Hz)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdEkcpX6CEnB",
        "outputId": "a8d96db4-2c99-43bb-ce81-7f8e9aff135a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vanilla DINOv2 Time for 281 Forward Props = 141.19125333629893 milliseconds (7.082591707137354 Hz)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Run this block to test the speed of the Iteration 0 Pipeline. Set speed\\_optimized=True for ORB Correspondence Speed and speed\\_optimized=False for ORB Correspondence Accuracy._"
      ],
      "metadata": {
        "id": "D2AWBeyO93d9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Speed Test - Iteration 0\n",
        "\n",
        "speed_optimized = False\n",
        "\n",
        "rho = 0.70\n",
        "\n",
        "start = torch.cuda.Event(enable_timing=True)\n",
        "end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "start.record()\n",
        "for i in range(num_images):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        original_dino_depth_map[i] = model.whole_inference(transformed_images[i], img_meta=None, rescale=True).squeeze()\n",
        "\n",
        "    if i > 0:\n",
        "        keypoints1, descriptors1 = feature_algorithm.detectAndCompute(images[i], None)\n",
        "        keypoints2, descriptors2 = feature_algorithm.detectAndCompute(images[i-1], None)\n",
        "\n",
        "        bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
        "        matches = bf.match(descriptors1, descriptors2)\n",
        "\n",
        "        if speed_optimized:\n",
        "            matches = [m for m in matches if m.distance < distance_threshold]\n",
        "\n",
        "        sum_tensor = torch.zeros_like(original_dino_depth_map[0])\n",
        "        count_tensor = torch.zeros_like(original_dino_depth_map[0])\n",
        "\n",
        "        for index in range(len(matches)):\n",
        "            match = matches[index]\n",
        "            img1_idx = match.queryIdx\n",
        "            img2_idx = match.trainIdx\n",
        "\n",
        "            (x1, y1) = keypoints1[img1_idx].pt\n",
        "            (x2, y2) = keypoints2[img2_idx].pt\n",
        "\n",
        "            x1 = int(x1)\n",
        "            y1 = int(y1)\n",
        "            x2 = int(x2)\n",
        "            y2 = int(y2)\n",
        "\n",
        "            y1_start = max(y1 - spatial_offset_pixels, 0)\n",
        "            y1_end = min(y1 + spatial_offset_pixels, image_height - 1)\n",
        "            x1_start = max(x1 - spatial_offset_pixels, 0)\n",
        "            x1_end = min(x1 + spatial_offset_pixels, image_width - 1)\n",
        "\n",
        "            y2_start = max(y2 - spatial_offset_pixels, 0)\n",
        "            y2_end = min(y2 + spatial_offset_pixels, image_height - 1)\n",
        "            x2_start = max(x2 - spatial_offset_pixels, 0)\n",
        "            x2_end = min(x2 + spatial_offset_pixels, image_width - 1)\n",
        "\n",
        "            if y1_start == 0:\n",
        "                y2_end = min(y2_end, y2_start + y1_end - y1_start)\n",
        "            if y2_start == 0:\n",
        "                y1_end = min(y1_end, y1_start + y2_end - y2_start)\n",
        "            if y1_end == image_height - 1:\n",
        "                y2_start = max(y2_start, y2_end - y1_end + y1_start)\n",
        "            if y2_end == image_height - 1:\n",
        "                y1_start = max(y1_start, y1_end - y2_end + y2_start)\n",
        "            if x1_start == 0:\n",
        "                x2_end = min(x2_end, x2_start + x1_end - x1_start)\n",
        "            if x2_start == 0:\n",
        "                x1_end = min(x1_end, x1_start + x2_end - x2_start)\n",
        "            if x1_end == image_width - 1:\n",
        "                x2_start = max(x2_start, x2_end - x1_end + x1_start)\n",
        "            if x2_end == image_width - 1:\n",
        "                x1_start = max(x1_start, x1_end - x2_end + x2_start)\n",
        "\n",
        "            # Extract valid subregion from original_dino_depth_map\n",
        "            subregion1 = original_dino_depth_map[i, y1_start:y1_end+1, x1_start:x1_end+1]\n",
        "            subregion2 = original_dino_depth_map[i-1, y2_start:y2_end+1, x2_start:x2_end+1]\n",
        "\n",
        "            sum_tensor[y1_start:y1_end+1, x1_start:x1_end+1] += rho * subregion1 + (1 - rho) * subregion2\n",
        "            count_tensor[y1_start:y1_end+1, x1_start:x1_end+1] += 1\n",
        "\n",
        "        updated_dino_depth_map[i] = original_dino_depth_map[i].clone()\n",
        "        updated_dino_depth_map[i][count_tensor != 0] = sum_tensor[count_tensor != 0] / count_tensor[count_tensor != 0]\n",
        "\n",
        "end.record()\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "avg_milliseconds_iteration_0 = start.elapsed_time(end)/num_images\n",
        "print(\"Iteration 0 Time for \" + str(num_images) + \" Forward Props = \" + str(avg_milliseconds_iteration_0) + \" milliseconds (\" + str(1000/avg_milliseconds_iteration_0) + \" Hz, \" + str(100*(avg_milliseconds_iteration_0-avg_milliseconds_original)/avg_milliseconds_original) + \" percent change relative to Vanilla DINOv2)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAsL_Pfm93Gw",
        "outputId": "dd24d31b-ed65-4037-9bc6-15634b211e50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0 Time for 281 Forward Props = 272.6891681494662 milliseconds (3.6671790331322613 Hz, 93.13460409615926 percent change relative to Vanilla DINOv2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Run this block to test the speed of the Iteration 1 Pipeline._"
      ],
      "metadata": {
        "id": "u9GdWy-UtqNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Speed Test - Iteration 1\n",
        "\n",
        "rho = 0.83\n",
        "\n",
        "start = torch.cuda.Event(enable_timing=True)\n",
        "end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "start.record()\n",
        "for i in range(num_images):\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        original_dino_depth_map[i] = model.whole_inference(images[i].unsqueeze(0).cuda(), img_meta=None, rescale=True).squeeze()\n",
        "\n",
        "    if i > 0:\n",
        "        shift_x, shift_y = cv2.phaseCorrelate(np.float32(grayscale_images[i-1]), np.float32(grayscale_images[i]))[0]\n",
        "\n",
        "        shift_x = round(shift_x)\n",
        "        shift_y = round(shift_y)\n",
        "\n",
        "        updated_dino_depth_map[i] = original_dino_depth_map[i].clone()\n",
        "\n",
        "        if shift_x >= 0 and shift_y >= 0:\n",
        "            updated_dino_depth_map[i, shift_y:, shift_x:] = rho * original_dino_depth_map[i, shift_y:, shift_x:] + (1 - rho) * original_dino_depth_map[i-1, :image_height-shift_y, :image_width-shift_x]\n",
        "        elif shift_x >= 0 and shift_y < 0:\n",
        "            updated_dino_depth_map[i, :image_height+shift_y, shift_x:] = rho * original_dino_depth_map[i, :image_height+shift_y, shift_x:] + (1 - rho) * original_dino_depth_map[i-1, -shift_y:, :image_width-shift_x]\n",
        "        elif shift_x < 0 and shift_y >= 0:\n",
        "            updated_dino_depth_map[i, shift_y:, :image_width+shift_x] = rho * original_dino_depth_map[i, shift_y:, :image_width+shift_x] + (1 - rho) * original_dino_depth_map[i-1, :image_height-shift_y, -shift_x:]\n",
        "        else:\n",
        "            updated_dino_depth_map[i, :image_height+shift_y, :image_width+shift_x] = rho * original_dino_depth_map[i, :image_height+shift_y, :image_width+shift_x] + (1 - rho) * original_dino_depth_map[i-1, -shift_y:, -shift_x:]\n",
        "\n",
        "end.record()\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "avg_milliseconds_iteration_1 = start.elapsed_time(end)/num_images\n",
        "print(\"Iteration 1 Time for \" + str(num_images) + \" Forward Props = \" + str(avg_milliseconds_iteration_1) + \" milliseconds (\" + str(1000/avg_milliseconds_iteration_1) + \" Hz, \" + str(100*(avg_milliseconds_iteration_1-avg_milliseconds_original)/avg_milliseconds_original) + \" percent change relative to Vanilla DINOv2)\")"
      ],
      "metadata": {
        "id": "SCckXDRZBxGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Run this block to test the accuracy of the Iteration 0 Pipeline, as compared to the Vanilla DINOv2 model. Set speed\\_optimized=True for ORB Correspondence Speed and speed\\_optimized=False for ORB Correspondence Accuracy._"
      ],
      "metadata": {
        "id": "ZTddqYZVvLAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy Test - Iteration 0\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "%cd '/content/'\n",
        "\n",
        "speed_optimized = False\n",
        "\n",
        "spatial_offset_pixels = 150\n",
        "\n",
        "rho = 0.70\n",
        "\n",
        "image_height = 480\n",
        "image_width = 640\n",
        "ground_truth_depth = np.zeros((num_images, image_height, image_width))\n",
        "original_dino_depth_map = torch.zeros((num_images, image_height, image_width))\n",
        "updated_dino_depth_map = torch.zeros((num_images, image_height, image_width))\n",
        "original_dino_mse = torch.zeros(num_images)\n",
        "updated_dino_mse = torch.zeros(num_images)\n",
        "difference = torch.zeros(num_images)\n",
        "images = []\n",
        "\n",
        "distance_threshold = 100\n",
        "\n",
        "for i in range(num_images):\n",
        "    images.append(cv2.imread(str(i+1) + '.jpg'))\n",
        "    ground_truth_depth[i] = 10.0 * np.array(Image.open(str(i+1) + '.png')).astype(float)/255.0\n",
        "\n",
        "feature_algorithm = cv2.ORB_create(nfeatures=500)\n",
        "transform = make_depth_transform()\n",
        "\n",
        "for i in range(num_images):\n",
        "    transformed_image = transform(images[i])\n",
        "    batch = transformed_image.unsqueeze(0).cuda()\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        original_dino_depth_map[i] = model.whole_inference(batch, img_meta=None, rescale=True).squeeze()\n",
        "\n",
        "    masked_true_depths = torch.tensor(ground_truth_depth[i][ground_truth_depth[i] > 0.0])\n",
        "    original_masked_dino_depths = torch.tensor(original_dino_depth_map[i][ground_truth_depth[i] > 0.0])\n",
        "\n",
        "    original_dino_mse[i] = F.mse_loss(original_masked_dino_depths, masked_true_depths)\n",
        "\n",
        "    print(\"\\nIteration 0 - Original Dino MSE for Image \" + str(i) + \" = \" + str(original_dino_mse[i].item()))\n",
        "\n",
        "    if i == 0:\n",
        "        updated_dino_mse[i] = original_dino_mse[i]\n",
        "        difference[i] = updated_dino_mse[i].item() - original_dino_mse[i].item()\n",
        "    else:\n",
        "        keypoints1, descriptors1 = feature_algorithm.detectAndCompute(images[i], None)\n",
        "        keypoints2, descriptors2 = feature_algorithm.detectAndCompute(images[i-1], None)\n",
        "\n",
        "        bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
        "        matches = bf.match(descriptors1, descriptors2)\n",
        "\n",
        "        if speed_optimized:\n",
        "            matches = [m for m in matches if m.distance < distance_threshold]\n",
        "\n",
        "        sum_tensor = torch.zeros_like(original_dino_depth_map[0])\n",
        "        count_tensor = torch.zeros_like(original_dino_depth_map[0])\n",
        "\n",
        "        for index in range(len(matches)):\n",
        "            match = matches[index]\n",
        "            img1_idx = match.queryIdx\n",
        "            img2_idx = match.trainIdx\n",
        "\n",
        "            (x1, y1) = keypoints1[img1_idx].pt\n",
        "            (x2, y2) = keypoints2[img2_idx].pt\n",
        "\n",
        "            x1 = int(x1)\n",
        "            y1 = int(y1)\n",
        "            x2 = int(x2)\n",
        "            y2 = int(y2)\n",
        "\n",
        "            y1_start = max(y1 - spatial_offset_pixels, 0)\n",
        "            y1_end = min(y1 + spatial_offset_pixels, image_height - 1)\n",
        "            x1_start = max(x1 - spatial_offset_pixels, 0)\n",
        "            x1_end = min(x1 + spatial_offset_pixels, image_width - 1)\n",
        "\n",
        "            y2_start = max(y2 - spatial_offset_pixels, 0)\n",
        "            y2_end = min(y2 + spatial_offset_pixels, image_height - 1)\n",
        "            x2_start = max(x2 - spatial_offset_pixels, 0)\n",
        "            x2_end = min(x2 + spatial_offset_pixels, image_width - 1)\n",
        "\n",
        "            if y1_start == 0:\n",
        "                y2_end = min(y2_end, y2_start + y1_end - y1_start)\n",
        "            if y2_start == 0:\n",
        "                y1_end = min(y1_end, y1_start + y2_end - y2_start)\n",
        "            if y1_end == image_height - 1:\n",
        "                y2_start = max(y2_start, y2_end - y1_end + y1_start)\n",
        "            if y2_end == image_height - 1:\n",
        "                y1_start = max(y1_start, y1_end - y2_end + y2_start)\n",
        "            if x1_start == 0:\n",
        "                x2_end = min(x2_end, x2_start + x1_end - x1_start)\n",
        "            if x2_start == 0:\n",
        "                x1_end = min(x1_end, x1_start + x2_end - x2_start)\n",
        "            if x1_end == image_width - 1:\n",
        "                x2_start = max(x2_start, x2_end - x1_end + x1_start)\n",
        "            if x2_end == image_width - 1:\n",
        "                x1_start = max(x1_start, x1_end - x2_end + x2_start)\n",
        "\n",
        "            # Extract valid subregion from original_dino_depth_map\n",
        "            subregion1 = original_dino_depth_map[i, y1_start:y1_end+1, x1_start:x1_end+1]\n",
        "            subregion2 = original_dino_depth_map[i-1, y2_start:y2_end+1, x2_start:x2_end+1]\n",
        "\n",
        "            sum_tensor[y1_start:y1_end+1, x1_start:x1_end+1] += rho * subregion1 + (1 - rho) * subregion2\n",
        "            count_tensor[y1_start:y1_end+1, x1_start:x1_end+1] += 1\n",
        "\n",
        "        average = original_dino_depth_map[i].clone()\n",
        "        average[count_tensor != 0] = sum_tensor[count_tensor != 0] / count_tensor[count_tensor != 0]\n",
        "\n",
        "        updated_dino_depth_map[i] = average.clone()\n",
        "\n",
        "        updated_masked_dino_depths = torch.tensor(updated_dino_depth_map[i][ground_truth_depth[i] > 0.0])\n",
        "\n",
        "        updated_dino_mse[i] = F.mse_loss(updated_masked_dino_depths, masked_true_depths)\n",
        "\n",
        "        print(\"Iteration 0 - Updated Dino MSE for Image \" + str(i) + \" = \" + str(updated_dino_mse[i].item()))\n",
        "\n",
        "        difference[i] = updated_dino_mse[i].item() - original_dino_mse[i].item()\n",
        "\n",
        "        print(\"Iteration 0 - Difference = \" + str(difference[i].item()))\n",
        "\n",
        "print(\"\\nIteration 0 - Mean Original DINO MSE Across \" + str(num_images) + \" Images = \" + str(torch.mean(original_dino_mse).item()))\n",
        "print(\"Iteration 0 - Mean Updated DINO MSE Across \" + str(num_images) + \" Images = \" + str(torch.mean(updated_dino_mse).item()))\n",
        "print(\"Iteration 0 - Mean Difference Across \" + str(num_images) + \" Images = \" + str(torch.mean(difference).item()) + \", (\" + str(torch.mean(difference).item()/torch.mean(original_dino_mse).item()*100) + \" percent change relative to Vanilla DINOv2)\")"
      ],
      "metadata": {
        "id": "M0HMR-GOvH6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Run this block to test the accuracy of the Iteration 1 Pipeline, as compared to the Vanilla DINOv2 model._"
      ],
      "metadata": {
        "id": "Kpcl5S7Htx9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy Test - Iteration 1\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "%cd '/content/'\n",
        "\n",
        "rho = 0.83\n",
        "\n",
        "image_height = 480\n",
        "image_width = 640\n",
        "ground_truth_depth = np.zeros((num_images, image_height, image_width))\n",
        "original_dino_depth_map = torch.zeros((num_images, image_height, image_width))\n",
        "updated_dino_depth_map = torch.zeros((num_images, image_height, image_width))\n",
        "original_dino_mse = torch.zeros(num_images)\n",
        "updated_dino_mse = torch.zeros(num_images)\n",
        "difference = torch.zeros(num_images)\n",
        "images = []\n",
        "grayscale_images = []\n",
        "\n",
        "for i in range(num_images):\n",
        "    images.append(cv2.imread(str(i+1) + '.jpg'))\n",
        "    grayscale_images.append(cv2.imread(str(i+1) + '.jpg', 0))\n",
        "    ground_truth_depth[i] = 10.0 * np.array(Image.open(str(i+1) + '.png')).astype(float)/255.0\n",
        "\n",
        "feature_algorithm = cv2.ORB_create(nfeatures=500)\n",
        "transform = make_depth_transform()\n",
        "\n",
        "for i in range(num_images):\n",
        "    transformed_image = transform(images[i])\n",
        "    batch = transformed_image.unsqueeze(0).cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        original_dino_depth_map[i] = model.whole_inference(batch, img_meta=None, rescale=True).squeeze()\n",
        "\n",
        "    masked_true_depths = torch.tensor(ground_truth_depth[i][ground_truth_depth[i] > 0.0])\n",
        "    original_masked_dino_depths = torch.tensor(original_dino_depth_map[i][ground_truth_depth[i] > 0.0])\n",
        "\n",
        "    original_dino_mse[i] = F.mse_loss(original_masked_dino_depths, masked_true_depths)\n",
        "\n",
        "    print(\"\\nIteration 1 - Original Dino MSE for Image \" + str(i) + \" = \" + str(original_dino_mse[i].item()))\n",
        "\n",
        "    if i == 0:\n",
        "        updated_dino_mse[i] = original_dino_mse[i]\n",
        "        difference[i] = 0.0\n",
        "    else:\n",
        "        shift_x, shift_y = cv2.phaseCorrelate(np.float32(grayscale_images[i-1]), np.float32(grayscale_images[i]))[0]\n",
        "\n",
        "        shift_x = round(shift_x)\n",
        "        shift_y = round(shift_y)\n",
        "\n",
        "        if np.abs(shift_x) < 100 and np.abs(shift_y) < 100:\n",
        "            updated_dino_depth_map[i] = original_dino_depth_map[i].clone()\n",
        "            if shift_x >= 0 and shift_y >= 0:\n",
        "                updated_dino_depth_map[i, shift_y:, shift_x:] = rho * original_dino_depth_map[i, shift_y:, shift_x:] + (1 - rho) * original_dino_depth_map[i-1, :image_height-shift_y, :image_width-shift_x]\n",
        "            elif shift_x >= 0 and shift_y < 0:\n",
        "                updated_dino_depth_map[i, :image_height+shift_y, shift_x:] = rho * original_dino_depth_map[i, :image_height+shift_y, shift_x:] + (1 - rho) * original_dino_depth_map[i-1, -shift_y:, :image_width-shift_x]\n",
        "            elif shift_x < 0 and shift_y >= 0:\n",
        "                updated_dino_depth_map[i, shift_y:, :image_width+shift_x] = rho * original_dino_depth_map[i, shift_y:, :image_width+shift_x] + (1 - rho) * original_dino_depth_map[i-1, :image_height-shift_y, -shift_x:]\n",
        "            else:\n",
        "                updated_dino_depth_map[i, :image_height+shift_y, :image_width+shift_x] = rho * original_dino_depth_map[i, :image_height+shift_y, :image_width+shift_x] + (1 - rho) * original_dino_depth_map[i-1, -shift_y:, -shift_x:]\n",
        "\n",
        "            updated_masked_dino_depths = torch.tensor(updated_dino_depth_map[i][ground_truth_depth[i] > 0.0])\n",
        "\n",
        "            updated_dino_mse[i] = F.mse_loss(updated_masked_dino_depths, masked_true_depths)\n",
        "\n",
        "            print(\"Iteration 1 - Updated Dino MSE for Image \" + str(i) + \" = \" + str(updated_dino_mse[i].item()))\n",
        "\n",
        "            difference[i] = updated_dino_mse[i].item() - original_dino_mse[i].item()\n",
        "\n",
        "            print(\"Iteration 1 - Difference = \" + str(difference[i].item()))\n",
        "        else:\n",
        "            updated_dino_mse[i] = original_dino_mse[i]\n",
        "            difference[i] = 0.0\n",
        "\n",
        "print(\"\\nIteration 0 - Mean Original DINO MSE Across \" + str(num_images) + \" Images = \" + str(torch.mean(original_dino_mse).item()))\n",
        "print(\"Iteration 0 - Mean Updated DINO MSE Across \" + str(num_images) + \" Images = \" + str(torch.mean(updated_dino_mse).item()))\n",
        "print(\"Iteration 0 - Mean Difference Across \" + str(num_images) + \" Images = \" + str(torch.mean(difference).item()) + \", (\" + str(torch.mean(difference).item()/torch.mean(original_dino_mse).item()*100) + \" percent change relative to Vanilla DINOv2)\")"
      ],
      "metadata": {
        "id": "hxwC2LTse3p5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}